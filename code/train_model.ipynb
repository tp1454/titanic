{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37d014b0",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning and model persistence overview\n",
    "\n",
    "This cell performs hyperparameter tuning for a RandomForest classifier using a robust cross-validation strategy and then serializes key outputs for downstream use. We begin by loading a preprocessed training dataset from `../result/processed/titanic_train_preprocessed.csv`, which should include engineered features, an integer target column named `Survived`, and an identifier `PassengerId`. The features `X` exclude both `Survived` and `PassengerId`, ensuring the model does not inadvertently learn from the ID and that the target remains separate. We set a `KFold` splitter with 10 folds, shuffled for stability, and establish a `SEED` to promote reproducibility across sampling operations.\n",
    "\n",
    "The parameter search space covers both structural and regularization dimensions: `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, and `max_features`, along with `bootstrap`, `oob_score`, and potential class weighting. `RandomizedSearchCV` (with cv=5) explores 30 random configurations, scoring by ROC AUC to balance sensitivity across classification thresholds. Inside the outer loop, we fit the random search on each training fold subset; this provides repeated assessments that can expose instability. After the loop, we evaluate the best estimator on the last validation split (a quick sanity check), compute accuracy and ROC AUC, and then persist both the tuning summary (best parameters and CV ROC AUC) and the tuned model itself to `../result/processed` via `joblib.dump`.\n",
    "\n",
    "Downstream notebooks can load `rf_best_all_features.pkl` to generate predictions against identically preprocessed test data. If you want a single final model fitted on the entire training set post-search (recommended for submission), you can call `.fit(X, y)` on `randomForest_best` before saving. Keep in mind that timing can be significant with larger grids; adjust `n_iter`, CV folds, or the parameter ranges to meet your runtime constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with cross-validation\n",
    "from utils import plot_feature_importances\n",
    "from sklearn.model_selection import RandomizedSearchCV , StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from scipy.stats import randint, uniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "PROCESS_PATH = '../result/processed'\n",
    "MODEL_PATH = '../result/model'\n",
    "PIC_PATH = '../result/pic'\n",
    "\n",
    "# Use the already prepared train_final (one-hot)\n",
    "train_final = pd.read_csv(f'{PROCESS_PATH}/titanic_train_preprocessed.csv')\n",
    "X = train_final.drop(columns=['Survived', 'PassengerId'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "accs, aucs = [], []\n",
    "\n",
    "for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    pred = rf.predict(X_valid)\n",
    "    proba = rf.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_valid, pred)\n",
    "    auc = roc_auc_score(y_valid, proba)\n",
    "    accs.append(acc)\n",
    "    aucs.append(auc)\n",
    "    print(f'Fold {fold_idx}: accuracy={acc:.4f}, roc_auc={auc:.4f}')\n",
    "\n",
    "print('\\nCV summary (10-fold):')\n",
    "print({'accuracy_mean': round(np.mean(accs), 4), 'accuracy_std': round(np.std(accs), 4),\n",
    "       'roc_auc_mean': round(np.mean(aucs), 4), 'roc_auc_std': round(np.std(aucs), 4)})\n",
    "\n",
    "# Refit on full data for deployment and save\n",
    "Base_rf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED,\n",
    ")\n",
    "Base_rf.fit(X, y)\n",
    "\n",
    "plot_feature_importances(Base_rf, X.columns, top_n=20, fname=f'{PIC_PATH}/rf_feature_importances_top20.png')\n",
    "import time\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(PROCESS_PATH, exist_ok=True)\n",
    "joblib.dump({'model': Base_rf, 'features': X.columns.tolist()}, f'{MODEL_PATH}/randomForest_Base_{timestamp}.pkl')\n",
    "print(f'Saved baseline model to {MODEL_PATH}/randomForest_Base_{timestamp}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbabf347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_final = pd.read_csv(f'{PROCESS_PATH}/titanic_train_preprocessed.csv')\n",
    "X = train_final.drop(columns=['Survived', 'PassengerId'])\n",
    "y = train_final['Survived']\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': randint(300, 900),\n",
    "    'max_depth': [None] + list(range(4, 13)),\n",
    "    'min_samples_split': randint(2, 11),\n",
    "    'min_samples_leaf': randint(1, 5),\n",
    "    'max_features': ['sqrt','log2', 0.5, 0.7],\n",
    "    'bootstrap': [True],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'oob_score': [True, False],\n",
    "}\n",
    "\n",
    "\n",
    "randomForest_tuner = RandomForestClassifier(n_jobs=-1, random_state=SEED)\n",
    "randomSearchCV = RandomizedSearchCV(\n",
    "    randomForest_tuner , param_distributions=param_dist, n_iter=100,\n",
    "    scoring='roc_auc', cv = StratifiedKFold(n_splits=10), random_state=SEED, n_jobs=-1, verbose=1, refit=True\n",
    ")\n",
    "\n",
    "randomSearchCV.fit(X_train, y_train)\n",
    "# print('score: ', randomSearchCV.cv_results_)\n",
    "\n",
    "randomForest_best = randomSearchCV.best_estimator_\n",
    "print('Best params:',  randomSearchCV.best_params_)\n",
    "print('Best CV ROC AUC:', round(randomSearchCV.best_score_,4))\n",
    "\n",
    "# Evaluate tuned model\n",
    "pred2 = randomForest_best.predict(X_valid)\n",
    "proba2 = randomForest_best.predict_proba(X_valid)[:,1]\n",
    "acc2 = accuracy_score(y_valid, pred2)\n",
    "auc2 = roc_auc_score(y_valid, proba2)\n",
    "\n",
    "\n",
    "print(f\"Tuned Random Forest - Accuracy: {acc2:.4f}, ROC AUC: {auc2:.4f}\")\n",
    "# Ensure output directory exists\n",
    "os.makedirs(PROCESS_PATH, exist_ok=True)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "plot_feature_importances(randomForest_best, X_train.columns, top_n=20, fname=f'{PIC_PATH}/rf_Best_feature_importances_top20.png')\n",
    "\n",
    "# Save tuning results with a clearer filename\n",
    "joblib.dump({'best_params': randomSearchCV.best_params_, 'best_cv_roc_auc': randomSearchCV.best_score_}, f\"{PROCESS_PATH}/rf_randomized_search_cv_results_{timestamp}.pkl\")\n",
    "print(f\"Saved tuning results to {MODEL_PATH}/rf_randomized_search_cv_results_{timestamp}.pkl\")\n",
    "\n",
    "# Save the best model trained on all features\n",
    "joblib.dump({'model': randomForest_best, 'features': X.columns.tolist()}, f\"{MODEL_PATH}/rf_best_all_features_{timestamp}.pkl\")\n",
    "print(f\"Saved tuned model to {MODEL_PATH}/rf_best_all_features_{timestamp}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load mutual information ranking computed in EDA\n",
    "\n",
    "mi_df = pd.read_csv(f'{PROCESS_PATH}/eda_mutual_information_top30.csv')\n",
    "mi_df = mi_df.rename(columns={mi_df.columns[0]: 'feature', mi_df.columns[1]: 'mi'})\n",
    "\n",
    "# Choose top-K features (intersection with training columns to be safe)\n",
    "TOP_K = 25\n",
    "topk = mi_df.sort_values('mi', ascending=False).head(TOP_K)['feature'].tolist()\n",
    "selected_cols = [c for c in topk if c in X.columns]\n",
    "if len(selected_cols) < max(10, TOP_K//2):\n",
    "    # Fallback: if few overlap (naming drift), just keep numeric + high-signal basics\n",
    "    baseline_keep = [c for c in X.columns if any(p in c for p in ['Sex','Pclass','Fare','Age','FamilySize','IsAlone','Embarked','Title'])]\n",
    "    selected_cols = sorted(set(selected_cols + baseline_keep))\n",
    "for c in selected_cols:\n",
    "    if c not in X.columns:\n",
    "        print(f'Warning: selected feature {c} not in training data columns.')\n",
    "print(f'Selected {len(selected_cols)} features for RF (EDA-informed).')\n",
    "\n",
    "X_train_sel = X_train[selected_cols].copy()\n",
    "X_valid_sel = X_valid[selected_cols].copy()\n",
    "\n",
    "# EDA-informed RF configuration (guided ranges from EDA)\n",
    "rf_eda = RandomForestClassifier(\n",
    "    n_estimators=randomSearchCV.best_params_['n_estimators'],\n",
    "    max_depth=randomSearchCV.best_params_['max_depth'],\n",
    "    min_samples_split=randomSearchCV.best_params_['min_samples_split'],\n",
    "    min_samples_leaf=randomSearchCV.best_params_['min_samples_leaf'],\n",
    "    max_features=randomSearchCV.best_params_['max_features'],\n",
    "    class_weight=randomSearchCV.best_params_['class_weight'],\n",
    "    bootstrap=randomSearchCV.best_params_['bootstrap'],\n",
    "    oob_score=randomSearchCV.best_params_['oob_score'],\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED\n",
    ")\n",
    "rf_eda.fit(X_train_sel, y_train)\n",
    "\n",
    "pred = rf_eda.predict(X_valid_sel)\n",
    "proba = rf_eda.predict_proba(X_valid_sel)[:,1]\n",
    "acc = accuracy_score(y_valid, pred)\n",
    "auc = roc_auc_score(y_valid, proba)\n",
    "oob = rf_eda.oob_score_ if rf_eda.oob_score else np.nan\n",
    "print({'mi_randomForest_accuracy': round(acc,4), 'mi_randomForest_roc_auc': round(auc,4)})\n",
    "print('\\nClassification report (EDA RF):\\n', classification_report(y_valid, pred, digits=3))\n",
    "print('\\nConfusion matrix (EDA RF):\\n', confusion_matrix(y_valid, pred))\n",
    "\n",
    "plot_feature_importances(rf_eda, X_train_sel.columns, top_n=20, fname=f'{PIC_PATH}/mi_best25_rf_feature_importances_top20.png')\n",
    "\n",
    "# Persist model and feature list\n",
    "os.makedirs(PROCESS_PATH, exist_ok=True)\n",
    "joblib.dump({'model': rf_eda, 'features': selected_cols}, f\"{MODEL_PATH}/mi_randomForest_{TOP_K}_{timestamp}_features.pkl\")\n",
    "print(f\"Saved EDA-tuned model to {MODEL_PATH}/mi_randomForest_{TOP_K}_{timestamp}_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18543474",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timestamp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
